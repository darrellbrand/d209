{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c8e581a-3b74-45fd-9cb5-71e12b193251",
   "metadata": {},
   "source": [
    "### A. \n",
    "\n",
    "### 1.\n",
    "   \n",
    "       Can we use KNN models to learn which customers are more likely to churn based on the data we have?\n",
    "\n",
    "### 2.\n",
    "\n",
    "    One goal of the data analysis is to identify customers that are more likely to churn, so the sales team can pro-actively offer them monthly sales specials.\n",
    "   \n",
    "### B.\n",
    "\n",
    "### 1.\n",
    "    KNN classifies unlabeled observations based on the nearest labeled observation's data points. By nearness we mean the most similar predictor variable values. An expected outcome would be: If the nearest K (chosen by analyst) number of data points to the new observation have churned, then the new unclassified observations will be classified to churn. If the nearest neighbors have not churned, the new observation will be classified as not churn.\n",
    "    This should happen for all predictions.\n",
    "\n",
    "### 2.\n",
    "\n",
    "    A core assumption of KNN is:\n",
    "     The closer two given points are to each other, the more related and similar they are. \n",
    "    \n",
    "      (Hachcham, 2023)\n",
    "\n",
    "\n",
    "### 3.\n",
    "\n",
    "    I have chosen python and the sklearn.neighbors.KNeighborsClassifier. \n",
    "\n",
    "    1) pandas will be used for transforming, manipulating, and cleaning data.\n",
    "\n",
    "    2) sklearn.neighbors.KNeighborsClassifier will be used to create the KNN classifier and make classification predictions.\n",
    "\n",
    "    3) sklearn.preprocessing import MinMaxScaler will be used to normalize numeric data.\n",
    "    \n",
    "    4) sklearn.model_selection import train_test_split will be used to split the training and test data.\n",
    "\n",
    "    5) sklearn.metrics import accuracy_score will be used to calculate accuracy of the model.\n",
    "\n",
    "    6) sklearn.metrics import roc_auc_score will be used to calculate the area under the curve score.\n",
    "\n",
    "### C.\n",
    "\n",
    "### 1. \n",
    "    One preprocessing goal for KNN would be normalization of data.\n",
    "\n",
    "### 2.\n",
    "\n",
    "    Predictors:\n",
    "    I will be using 'Age','Income','Outage_sec_perweek','Contacts','Yearly_equip_failure','Tenure','Bandwidth_GB_Year','Age' which are continuous. \n",
    "\n",
    "    I will also be using 'Gender','Area','InternetService','Phone','OnlineSecurity','DeviceProtection','StreamingMovies',and 'OnlineBackup' which are categorical.\n",
    "\n",
    "    Predicted:\n",
    "    Predicted variable will be churn which is categorical.\n",
    "\n",
    "\n",
    "### 3. \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f86cac0-e9f8-404d-a0b9-fcd7bc64a799",
   "metadata": {},
   "source": [
    "####  read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3bd30db-1b26-4125-8467-855fb1512070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries and read in the data from file.\n",
    "import pandas as pd\n",
    "# read in the data\n",
    "file_path = '/home/dj/skewl/d209/churn_clean.csv'\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Read the data from the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "#drop index column\n",
    "df = df.loc[:, ~df.columns.str.contains('Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184df862-15a3-46b2-80fb-fb922ecf7cd6",
   "metadata": {},
   "source": [
    "#### find duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ace597-cd78-4440-82d1-005aa7309bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Find duplicate rows \n",
    "duplicate_rows = df.duplicated([\"CaseOrder\"]).sum()\n",
    "\n",
    "# Print duplicate rows   # found NO duplicate rows here!\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada80fc1-70fa-4df1-9a62-60b0401b9bd8",
   "metadata": {},
   "source": [
    "### identify missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8384fc-2c05-4e14-8e5d-d055cf014ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CaseOrder               0\n",
      "Customer_id             0\n",
      "Interaction             0\n",
      "UID                     0\n",
      "City                    0\n",
      "State                   0\n",
      "County                  0\n",
      "Zip                     0\n",
      "Lat                     0\n",
      "Lng                     0\n",
      "Population              0\n",
      "Area                    0\n",
      "TimeZone                0\n",
      "Job                     0\n",
      "Children                0\n",
      "Age                     0\n",
      "Income                  0\n",
      "Marital                 0\n",
      "Gender                  0\n",
      "Churn                   0\n",
      "Outage_sec_perweek      0\n",
      "Email                   0\n",
      "Contacts                0\n",
      "Yearly_equip_failure    0\n",
      "Techie                  0\n",
      "Contract                0\n",
      "Port_modem              0\n",
      "Tablet                  0\n",
      "InternetService         0\n",
      "Phone                   0\n",
      "Multiple                0\n",
      "OnlineSecurity          0\n",
      "OnlineBackup            0\n",
      "DeviceProtection        0\n",
      "TechSupport             0\n",
      "StreamingTV             0\n",
      "StreamingMovies         0\n",
      "PaperlessBilling        0\n",
      "PaymentMethod           0\n",
      "Tenure                  0\n",
      "MonthlyCharge           0\n",
      "Bandwidth_GB_Year       0\n",
      "Item1                   0\n",
      "Item2                   0\n",
      "Item3                   0\n",
      "Item4                   0\n",
      "Item5                   0\n",
      "Item6                   0\n",
      "Item7                   0\n",
      "Item8                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify missing values using isna() method\n",
    "missing_values = df.isna().sum()\n",
    "# Print DataFrame with True for missing values and False for non-missing values\n",
    "print(missing_values)\n",
    "\n",
    "# no missing values here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf2d04-ba91-40e6-8247-24f925df4eac",
   "metadata": {},
   "source": [
    "### Check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c72b86be-2185-4de4-8d4b-bb2d448fd474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CaseOrder</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lng</th>\n",
       "      <th>Population</th>\n",
       "      <th>Children</th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Outage_sec_perweek</th>\n",
       "      <th>Email</th>\n",
       "      <th>Contacts</th>\n",
       "      <th>Yearly_equip_failure</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>MonthlyCharge</th>\n",
       "      <th>Bandwidth_GB_Year</th>\n",
       "      <th>Item1</th>\n",
       "      <th>Item2</th>\n",
       "      <th>Item3</th>\n",
       "      <th>Item4</th>\n",
       "      <th>Item5</th>\n",
       "      <th>Item6</th>\n",
       "      <th>Item7</th>\n",
       "      <th>Item8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>49153.319600</td>\n",
       "      <td>38.757567</td>\n",
       "      <td>-90.782536</td>\n",
       "      <td>9756.562400</td>\n",
       "      <td>2.0877</td>\n",
       "      <td>53.078400</td>\n",
       "      <td>39806.926771</td>\n",
       "      <td>10.001848</td>\n",
       "      <td>12.016000</td>\n",
       "      <td>0.994200</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>34.526188</td>\n",
       "      <td>172.624816</td>\n",
       "      <td>3392.341550</td>\n",
       "      <td>3.490800</td>\n",
       "      <td>3.505100</td>\n",
       "      <td>3.487000</td>\n",
       "      <td>3.497500</td>\n",
       "      <td>3.492900</td>\n",
       "      <td>3.497300</td>\n",
       "      <td>3.509500</td>\n",
       "      <td>3.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.89568</td>\n",
       "      <td>27532.196108</td>\n",
       "      <td>5.437389</td>\n",
       "      <td>15.156142</td>\n",
       "      <td>14432.698671</td>\n",
       "      <td>2.1472</td>\n",
       "      <td>20.698882</td>\n",
       "      <td>28199.916702</td>\n",
       "      <td>2.976019</td>\n",
       "      <td>3.025898</td>\n",
       "      <td>0.988466</td>\n",
       "      <td>0.635953</td>\n",
       "      <td>26.443063</td>\n",
       "      <td>42.943094</td>\n",
       "      <td>2185.294852</td>\n",
       "      <td>1.037797</td>\n",
       "      <td>1.034641</td>\n",
       "      <td>1.027977</td>\n",
       "      <td>1.025816</td>\n",
       "      <td>1.024819</td>\n",
       "      <td>1.033586</td>\n",
       "      <td>1.028502</td>\n",
       "      <td>1.028633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>17.966120</td>\n",
       "      <td>-171.688150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>348.670000</td>\n",
       "      <td>0.099747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000259</td>\n",
       "      <td>79.978860</td>\n",
       "      <td>155.506715</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2500.75000</td>\n",
       "      <td>26292.500000</td>\n",
       "      <td>35.341828</td>\n",
       "      <td>-97.082812</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>19224.717500</td>\n",
       "      <td>8.018214</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.917694</td>\n",
       "      <td>139.979239</td>\n",
       "      <td>1236.470827</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>48869.500000</td>\n",
       "      <td>39.395800</td>\n",
       "      <td>-87.918800</td>\n",
       "      <td>2910.500000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>33170.605000</td>\n",
       "      <td>10.018560</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.430507</td>\n",
       "      <td>167.484700</td>\n",
       "      <td>3279.536903</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7500.25000</td>\n",
       "      <td>71866.500000</td>\n",
       "      <td>42.106908</td>\n",
       "      <td>-80.088745</td>\n",
       "      <td>13168.000000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>53246.170000</td>\n",
       "      <td>11.969485</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>61.479795</td>\n",
       "      <td>200.734725</td>\n",
       "      <td>5586.141370</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>99929.000000</td>\n",
       "      <td>70.640660</td>\n",
       "      <td>-65.667850</td>\n",
       "      <td>111850.000000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>258900.700000</td>\n",
       "      <td>21.207230</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>71.999280</td>\n",
       "      <td>290.160419</td>\n",
       "      <td>7158.981530</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CaseOrder           Zip           Lat           Lng     Population  \\\n",
       "count  10000.00000  10000.000000  10000.000000  10000.000000   10000.000000   \n",
       "mean    5000.50000  49153.319600     38.757567    -90.782536    9756.562400   \n",
       "std     2886.89568  27532.196108      5.437389     15.156142   14432.698671   \n",
       "min        1.00000    601.000000     17.966120   -171.688150       0.000000   \n",
       "25%     2500.75000  26292.500000     35.341828    -97.082812     738.000000   \n",
       "50%     5000.50000  48869.500000     39.395800    -87.918800    2910.500000   \n",
       "75%     7500.25000  71866.500000     42.106908    -80.088745   13168.000000   \n",
       "max    10000.00000  99929.000000     70.640660    -65.667850  111850.000000   \n",
       "\n",
       "         Children           Age         Income  Outage_sec_perweek  \\\n",
       "count  10000.0000  10000.000000   10000.000000        10000.000000   \n",
       "mean       2.0877     53.078400   39806.926771           10.001848   \n",
       "std        2.1472     20.698882   28199.916702            2.976019   \n",
       "min        0.0000     18.000000     348.670000            0.099747   \n",
       "25%        0.0000     35.000000   19224.717500            8.018214   \n",
       "50%        1.0000     53.000000   33170.605000           10.018560   \n",
       "75%        3.0000     71.000000   53246.170000           11.969485   \n",
       "max       10.0000     89.000000  258900.700000           21.207230   \n",
       "\n",
       "              Email      Contacts  Yearly_equip_failure        Tenure  \\\n",
       "count  10000.000000  10000.000000          10000.000000  10000.000000   \n",
       "mean      12.016000      0.994200              0.398000     34.526188   \n",
       "std        3.025898      0.988466              0.635953     26.443063   \n",
       "min        1.000000      0.000000              0.000000      1.000259   \n",
       "25%       10.000000      0.000000              0.000000      7.917694   \n",
       "50%       12.000000      1.000000              0.000000     35.430507   \n",
       "75%       14.000000      2.000000              1.000000     61.479795   \n",
       "max       23.000000      7.000000              6.000000     71.999280   \n",
       "\n",
       "       MonthlyCharge  Bandwidth_GB_Year         Item1         Item2  \\\n",
       "count   10000.000000       10000.000000  10000.000000  10000.000000   \n",
       "mean      172.624816        3392.341550      3.490800      3.505100   \n",
       "std        42.943094        2185.294852      1.037797      1.034641   \n",
       "min        79.978860         155.506715      1.000000      1.000000   \n",
       "25%       139.979239        1236.470827      3.000000      3.000000   \n",
       "50%       167.484700        3279.536903      3.000000      4.000000   \n",
       "75%       200.734725        5586.141370      4.000000      4.000000   \n",
       "max       290.160419        7158.981530      7.000000      7.000000   \n",
       "\n",
       "              Item3         Item4         Item5         Item6         Item7  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       3.487000      3.497500      3.492900      3.497300      3.509500   \n",
       "std        1.027977      1.025816      1.024819      1.033586      1.028502   \n",
       "min        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "25%        3.000000      3.000000      3.000000      3.000000      3.000000   \n",
       "50%        3.000000      3.000000      3.000000      3.000000      4.000000   \n",
       "75%        4.000000      4.000000      4.000000      4.000000      4.000000   \n",
       "max        8.000000      7.000000      7.000000      8.000000      7.000000   \n",
       "\n",
       "              Item8  \n",
       "count  10000.000000  \n",
       "mean       3.495600  \n",
       "std        1.028633  \n",
       "min        1.000000  \n",
       "25%        3.000000  \n",
       "50%        3.000000  \n",
       "75%        4.000000  \n",
       "max        8.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52603b09-1404-4df3-bf23-5165da926def",
   "metadata": {},
   "source": [
    "### encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25f063a-a655-4ea3-ad7a-b0788b8f610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split continuous and categorical variables into separate dataframes\n",
    "dfcon = df[['Age','Income','Outage_sec_perweek','Contacts','Yearly_equip_failure','Tenure','Bandwidth_GB_Year','Age']]\n",
    "dfcat = df[['Gender','Area','InternetService','Phone','OnlineSecurity','DeviceProtection','StreamingMovies','OnlineBackup']]\n",
    "#split data into x and y\n",
    "y = df['Churn']\n",
    "#one-hot encode categorical data and drop first level of each\n",
    "dfcat_encoded = pd.get_dummies(dfcat,drop_first=True)\n",
    "#concatenate the columns\n",
    "x = pd.concat([dfcon, dfcat_encoded], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd8101-424b-40ec-8683-f1b4bc80e771",
   "metadata": {},
   "source": [
    "#### normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13118942-deb2-4140-b6ba-6dde2bf713fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data after encoding\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(x), columns=x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f31a7f1a-99a3-44d1-9453-64d6b72a0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the prepared data to .csv file\n",
    "x['Churn'] = y\n",
    "x.to_csv('prepared-data.csv', index=False)\n",
    "del x['Churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221b6c4a-56bb-49c7-a5e1-1aaf615bcef3",
   "metadata": {},
   "source": [
    "### D.\n",
    " ### 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dea170e-986e-4347-8a8c-e27bd3682c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "## split into training and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=35)\n",
    "# write to csv\n",
    "X_train.to_csv('x_train.csv', index=False)\n",
    "X_test.to_csv('x_test.csv', index=False)\n",
    "Y_train.to_csv('y_train.csv', index=False)\n",
    "Y_test.to_csv('y_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08126756-596c-47da-baff-f1a76858c177",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "I am using KNN classification with a K value of 5 to train a model that will predict which customers are more likely to churn.\n",
    "\n",
    "I import the library from sklearn.neighbors import KNeighborsClassifier. Then I instantiate the classification model with the k=5. Then I fit the model with the training predictor variables, and training dependent variable data. Lastly I predict the classifications for the test data. \n",
    "\n",
    "\n",
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5595d1f-8ffd-4825-904b-b249adc43a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "k = 5  # Number of neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Train the KNN classifier\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d45b947e-5757-4373-bdb7-d163c0e0188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bb2c859-9ddc-433c-a121-7008571de791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7380209872510746\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Predict the probabilities of the positive class for the test data\n",
    "Y_probs = knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute the AUC score\n",
    "auc_score = roc_auc_score(Y_test, Y_probs)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57675954-c475-42fc-9e9a-bd3fceebfd0b",
   "metadata": {},
   "source": [
    "### E.\n",
    "\n",
    "### 1.\n",
    "\n",
    "     The accuracy of my model is  0.731. This means that it correctly predicted 73% of of all predictions made.\n",
    "\n",
    "     The AUC of my model is 0.7602. This means that across all probability threshold values it was able to distinguish between negative and positive classes at a score of 0.76.\n",
    "     The AUC measurement of pure chance is 0.5.\n",
    "\n",
    "### 2.\n",
    "\n",
    "     My classification model can predict if a customer will churn with 73% accuracy. It also has an AUC score of 76% which is 26% better than random chance. The implications are that we can with some level of statistical certainty predict which customers are going to churn.\n",
    "\n",
    "\n",
    "### 3.\n",
    "    One limit of this data analysis is that the model isn't as accurate as I would like it to be. \n",
    "\n",
    "### 4.\n",
    "    I think we should use this model to predict which customers are going to churn and have the sales team reach out and offer sales and promotions to the customers that the KNN model identified as likely to churn to mitigate churn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a51bc-e081-4ded-a66f-43cf488cf8df",
   "metadata": {},
   "source": [
    "### citations\n",
    "\n",
    "Hachcham, A. (2023, August 11). The KNN Algorithm &#8211; Explanation, Opportunities, Limitations. neptune.ai. https://neptune.ai/blog/knn-algorithm-explanation-opportunities-limitations#:~:text=A%20core%20assumption%20of%20KNN,related%20and%20similar%20they%20are.&text=Typically%20used%20with%20data%20transmitted,also%20used%20with%20categorical%20variables.\n",
    "\n",
    "K-nearest Neighbors (KNN) Classification Model. (n.d.). ritchieng.github.io. https://www.ritchieng.com/machine-learning-k-nearest-neighbors-knn/\n",
    "\n",
    "  Srivastava, T. (2024, January 4). A Complete Guide to K-Nearest Neighbors (Updated 2024). Analytics Vidhya. https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8022696-f184-4cee-8882-6c6a21e28c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
